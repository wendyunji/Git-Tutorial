{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "matrixfactorization2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMboz+hM83sZhESsk20rUm5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wendyunji/Git-Tutorial/blob/master/matrixfactorization2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snChT-3gxYsz",
        "outputId": "d8c6a663-023d-4498-c588-76fd329e7234"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MatrixFactorization():\n",
        "    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n",
        "        \"\"\"\n",
        "        :param R: rating matrix\n",
        "        :param k: latent parameter\n",
        "        :param learning_rate: alpha on weight update\n",
        "        :param reg_param: beta on weight update\n",
        "        :param epochs: training epochs\n",
        "        :param verbose: print status\n",
        "        \"\"\"\n",
        "\n",
        "        self._R = R\n",
        "        self._num_users, self._num_items = R.shape\n",
        "        self._k = k\n",
        "        self._learning_rate = learning_rate\n",
        "        self._reg_param = reg_param\n",
        "        self._epochs = epochs\n",
        "        self._verbose = verbose\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        training Matrix Factorization : Update matrix latent weight and bias\n",
        "\n",
        "        참고: self._b에 대한 설명\n",
        "        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n",
        "        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n",
        "\n",
        "        :return: training_process\n",
        "        \"\"\"\n",
        "\n",
        "        # init latent features\n",
        "        self._P = np.random.normal(size=(self._num_users, self._k))\n",
        "        self._Q = np.random.normal(size=(self._num_items, self._k))\n",
        "\n",
        "        # init biases\n",
        "        self._b_P = np.zeros(self._num_users)\n",
        "        self._b_Q = np.zeros(self._num_items)\n",
        "        self._b = np.mean(self._R[np.where(self._R != 0)])\n",
        "\n",
        "        # train while epochs\n",
        "        self._training_process = []\n",
        "        for epoch in range(self._epochs):\n",
        "\n",
        "            # rating이 존재하는 index를 기준으로 training\n",
        "            for i in range(self._num_users):\n",
        "                for j in range(self._num_items):\n",
        "                    if self._R[i, j] > 0:\n",
        "                        self.gradient_descent(i, j, self._R[i, j])\n",
        "            cost = self.cost()\n",
        "            self._training_process.append((epoch, cost))\n",
        "\n",
        "            # print status\n",
        "            if self._verbose == True and ((epoch + 1) % 10 == 0):\n",
        "                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n",
        "\n",
        "\n",
        "    def cost(self):\n",
        "        \"\"\"\n",
        "        compute root mean square error\n",
        "        :return: rmse cost\n",
        "        \"\"\"\n",
        "\n",
        "        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n",
        "        # 참고: http://codepractice.tistory.com/90\n",
        "        xi, yi = self._R.nonzero()\n",
        "        predicted = self.get_complete_matrix()\n",
        "        cost = 0\n",
        "        for x, y in zip(xi, yi):\n",
        "            cost += pow(self._R[x, y] - predicted[x, y], 2)\n",
        "        return np.sqrt(cost) / len(xi)\n",
        "\n",
        "\n",
        "    def gradient(self, error, i, j):\n",
        "        \"\"\"\n",
        "        gradient of latent feature for GD\n",
        "\n",
        "        :param error: rating - prediction error\n",
        "        :param i: user index\n",
        "        :param j: item index\n",
        "        :return: gradient of latent feature tuple\n",
        "        \"\"\"\n",
        "\n",
        "        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n",
        "        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n",
        "        return dp, dq\n",
        "\n",
        "\n",
        "    def gradient_descent(self, i, j, rating):\n",
        "        \"\"\"\n",
        "        graident descent function\n",
        "\n",
        "        :param i: user index of matrix\n",
        "        :param j: item index of matrix\n",
        "        :param rating: rating of (i,j)\n",
        "        \"\"\"\n",
        "\n",
        "        # get error\n",
        "        prediction = self.get_prediction(i, j)\n",
        "        error = rating - prediction\n",
        "\n",
        "        # update biases\n",
        "        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n",
        "        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n",
        "\n",
        "        # update latent feature\n",
        "        dp, dq = self.gradient(error, i, j)\n",
        "        self._P[i, :] += self._learning_rate * dp\n",
        "        self._Q[j, :] += self._learning_rate * dq\n",
        "\n",
        "\n",
        "    def get_prediction(self, i, j):\n",
        "        \"\"\"\n",
        "        get predicted rating: user_i, item_j\n",
        "        :return: prediction of r_ij\n",
        "        \"\"\"\n",
        "        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n",
        "\n",
        "\n",
        "    def get_complete_matrix(self):\n",
        "        \"\"\"\n",
        "        computer complete matrix PXQ + P.bias + Q.bias + global bias\n",
        "\n",
        "        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n",
        "        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n",
        "        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n",
        "\n",
        "        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n",
        "\n",
        "        :return: complete matrix R^\n",
        "        \"\"\"\n",
        "        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n",
        "\n",
        "\n",
        "    def print_results(self):\n",
        "        \"\"\"\n",
        "        print fit results\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"User Latent P:\")\n",
        "        print(self._P)\n",
        "        print(\"Item Latent Q:\")\n",
        "        print(self._Q.T)\n",
        "        print(\"P x Q:\")\n",
        "        print(self._P.dot(self._Q.T))\n",
        "        print(\"bias:\")\n",
        "        print(self._b)\n",
        "        print(\"User Latent bias:\")\n",
        "        print(self._b_P)\n",
        "        print(\"Item Latent bias:\")\n",
        "        print(self._b_Q)\n",
        "        print(\"Final R matrix:\")\n",
        "        print(self.get_complete_matrix())\n",
        "        print(\"Final RMSE:\")\n",
        "        print(self._training_process[self._epochs-1][1])\n",
        "\n",
        "\n",
        "# run example\n",
        "if __name__ == \"__main__\":\n",
        "    # rating matrix - User X Item : (7 X 5)\n",
        "    R = np.array([\n",
        "        [8, 0, 0, 0, 0],\n",
        "        [0, 1, 0, 0, 0],\n",
        "        [0, 0, 1, 0, 0],\n",
        "        [0, 0, 0, 4, 0],\n",
        "        [0, 0, 0, 0, 1],\n",
        "    ])\n",
        "\n",
        "    # P, Q is (7 X k), (k X 5) matrix\n",
        "    factorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=300, verbose=True)\n",
        "    factorizer.fit()\n",
        "    factorizer.print_results()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 10 ; cost = 0.4631\n",
            "Iteration: 20 ; cost = 0.2184\n",
            "Iteration: 30 ; cost = 0.1091\n",
            "Iteration: 40 ; cost = 0.0568\n",
            "Iteration: 50 ; cost = 0.0308\n",
            "Iteration: 60 ; cost = 0.0174\n",
            "Iteration: 70 ; cost = 0.0104\n",
            "Iteration: 80 ; cost = 0.0066\n",
            "Iteration: 90 ; cost = 0.0046\n",
            "Iteration: 100 ; cost = 0.0035\n",
            "Iteration: 110 ; cost = 0.0029\n",
            "Iteration: 120 ; cost = 0.0027\n",
            "Iteration: 130 ; cost = 0.0025\n",
            "Iteration: 140 ; cost = 0.0024\n",
            "Iteration: 150 ; cost = 0.0024\n",
            "Iteration: 160 ; cost = 0.0024\n",
            "Iteration: 170 ; cost = 0.0023\n",
            "Iteration: 180 ; cost = 0.0023\n",
            "Iteration: 190 ; cost = 0.0023\n",
            "Iteration: 200 ; cost = 0.0023\n",
            "Iteration: 210 ; cost = 0.0023\n",
            "Iteration: 220 ; cost = 0.0023\n",
            "Iteration: 230 ; cost = 0.0023\n",
            "Iteration: 240 ; cost = 0.0023\n",
            "Iteration: 250 ; cost = 0.0023\n",
            "Iteration: 260 ; cost = 0.0023\n",
            "Iteration: 270 ; cost = 0.0023\n",
            "Iteration: 280 ; cost = 0.0024\n",
            "Iteration: 290 ; cost = 0.0024\n",
            "Iteration: 300 ; cost = 0.0024\n",
            "User Latent P:\n",
            "[[-1.3811356  -0.16539759  1.48973467]\n",
            " [ 0.59305955 -0.33875466  1.0890331 ]\n",
            " [-0.57456583 -0.78190778 -0.29517677]\n",
            " [-1.32602653 -0.6451818   0.3747572 ]\n",
            " [ 1.65216581  0.34886985  0.52476421]]\n",
            "Item Latent Q:\n",
            "[[-2.51025825  0.24188094  0.93615079 -0.62966667 -1.23668342]\n",
            " [ 0.54470596 -1.42610545  0.7104036   0.73650157 -0.42225765]\n",
            " [ 0.78708888 -1.41152911  0.99024685  0.54852006  0.74172782]]\n",
            "P x Q:\n",
            "[[ 4.54946757 -2.20099981  0.06475484  1.56498881  2.88284554]\n",
            " [-0.81608845 -0.91065226  1.39295223 -0.02556666  0.21738099]\n",
            " [ 0.78406843  1.39275702 -1.38564822 -0.37600173  0.82178176]\n",
            " [ 3.27220189  0.07037605 -1.32859813  0.56533914  2.19027582]\n",
            " [-3.54429529 -0.83861774  2.31416083 -0.49552685 -1.80128682]]\n",
            "bias:\n",
            "3.0\n",
            "User Latent bias:\n",
            "[ 0.22169976 -0.54279871 -0.30404358  0.21592665 -0.0967888 ]\n",
            "Item Latent bias:\n",
            "[ 0.22169976 -0.54279871 -0.30404358  0.21592665 -0.0967888 ]\n",
            "Final R matrix:\n",
            "[[ 7.99286709  0.47790124  2.98241103  5.00261523  6.00775651]\n",
            " [ 1.86281261  1.00375033  3.54610995  2.64756128  2.57779349]\n",
            " [ 3.70172462  3.54591474  1.00626463  2.53588134  3.42094938]\n",
            " [ 6.7098283   2.743504    1.58328494  3.99719243  5.30941367]\n",
            " [-0.41938433  1.52179476  4.91332846  2.623611    1.00513559]]\n",
            "Final RMSE:\n",
            "0.0023532583916817543\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}